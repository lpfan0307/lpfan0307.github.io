<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="life of nullfan">
<meta property="og:url" content="http://lpfan0307.com/index.html">
<meta property="og:site_name" content="life of nullfan">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="nullfan">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://lpfan0307.com/"/>





  <title>life of nullfan</title>
  








<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">life of nullfan</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://lpfan0307.com/2020/03/01/ch1-Recommender-Systems-An-Introduction-to-Recommender-Systems/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="nullfan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="life of nullfan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/01/ch1-Recommender-Systems-An-Introduction-to-Recommender-Systems/" itemprop="url"> Recommender Systems-ch1 An Introduction to Recommender Systems</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-03-01T11:14:00+08:00">
                2020-03-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="推荐系统的主要目标："><a href="#推荐系统的主要目标：" class="headerlink" title="推荐系统的主要目标："></a>推荐系统的主要目标：</h1><ul>
<li>Relevance:相关性，推荐和用户相关的</li>
<li>Novely:和用户历史行为无关，但却是用户兴趣的item</li>
<li>Serendipity:这个主要用来探索用户可能存在的兴趣的item</li>
<li>Increasing recommendation diversity:对于top-k的item，如果都是某个类，那么user很可能对这些类都不感兴趣； 但如果这top-k的item来自不同的类，那么用户很有可能对其中某个item感兴趣  </li>
</ul>
<h1 id="basic-models-of-recommender-systems"><a href="#basic-models-of-recommender-systems" class="headerlink" title="basic models of recommender systems"></a>basic models of recommender systems</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Collaborative Filtering Models</span><br></pre></td></tr></table></figure>
<ul>
<li>simple to implememt and easy to explain  </li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Content-Based Recommender Systems</span><br></pre></td></tr></table></figure>
<ul>
<li>have some advantage for new items when sufficient rating data not available for that item  </li>
<li>provide obvious recommendations, reduce the diversity of the recommended items  </li>
<li>not effective at providing recommendations for new users </li>
<li>适用场景：the season and the location of the user; a particular type of festival or holiday  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Knowledge-Based Recommender Systems</span><br></pre></td></tr></table></figure></li>
<li>particularly userful in the context of items that are not purchased very often(luxury goods etc.)  </li>
<li>inherit some of same disadvantage of content-based systems</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://lpfan0307.com/2020/02/28/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chp11-chp12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="nullfan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="life of nullfan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/28/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chp11-chp12/" itemprop="url"> Deep Learning读书笔记 chp11 & chp12</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-02-28T23:43:17+08:00">
                2020-02-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="practical-design-process"><a href="#practical-design-process" class="headerlink" title="practical design process"></a>practical design process</h1><h2 id="determine-your-goals"><a href="#determine-your-goals" class="headerlink" title="determine your goals"></a>determine your goals</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">performance metric 和 cost function</span><br></pre></td></tr></table></figure>
<p>通常情况下，performance metric和cost function是不相同的，通常使用准确率、错误率等来衡量一个系统，而使用log loss等来作为cost function  </p>
<h2 id="establish-a-working-end-to-end-pipeline-as-soon-as-possible"><a href="#establish-a-working-end-to-end-pipeline-as-soon-as-possible" class="headerlink" title="establish a working end-to-end pipeline as soon as possible"></a>establish a working end-to-end pipeline as soon as possible</h2><p>通常使用SGD（with momentum，with a deaying learning rate）来作为baseline  </p>
<h2 id="instrument-the-system-well-to-determine-bottlenecks-in-performance"><a href="#instrument-the-system-well-to-determine-bottlenecks-in-performance" class="headerlink" title="instrument the system well to determine bottlenecks in performance"></a>instrument the system well to determine bottlenecks in performance</h2><h2 id="repeatly-make-incremental-changes"><a href="#repeatly-make-incremental-changes" class="headerlink" title="repeatly make incremental changes"></a>repeatly make incremental changes</h2><h3 id="是否需要获取更多数据"><a href="#是否需要获取更多数据" class="headerlink" title="是否需要获取更多数据"></a>是否需要获取更多数据</h3><p>虽然可以采用不同的算法来提升系统，但获取更多的数据经常是一个更好的做法，可以通过画出training set size和generation error的方式来看出是否需要更多的训练数据  </p>
<h3 id="参数调整"><a href="#参数调整" class="headerlink" title="参数调整"></a>参数调整</h3><p>学习率几乎是最终要的参数，如果只能够调整一个参数，那么就调整学习率<br>其他参数的调整需要通过观察训练误差和泛化误差来做出结论<br>如果训练集误差比泛化误差大，则模型是under fitting，需要增加模型的复杂度；如果训练集误差比泛化误差小，通常可以在正则项做一些文章。  </p>
<h3 id="grid-search-VS-random-search"><a href="#grid-search-VS-random-search" class="headerlink" title="grid search VS random search"></a>grid search VS random search</h3><p><img src="/images/pasted-79.png" alt="upload successful"><br>一个模型通常只对几个特定的参数敏感，例如图中该模型只对横坐标的值敏感，使用grid search会重复多次相同横坐标的值而浪费计算资源。使用random search可以取得更好的效果，random search的一个例子如下：<br>log_learning_rate ~ $u(-1, -5)$<br>learning_rate = $10^{log_learning_rate}$  </p>
<h3 id="debugging-strategies"><a href="#debugging-strategies" class="headerlink" title="debugging strategies"></a>debugging strategies</h3><ul>
<li>visualize the model in action，例如可以观察模型中隐藏节点的激活比例等等    </li>
<li>visual the worst mistakes，这个是个很好的想法，通常看模型最不确定的正样本可以获取不少的insight，不论是模型的学习效果还是特征的补充  <h3 id="不同年份都区分开的语言模型"><a href="#不同年份都区分开的语言模型" class="headerlink" title="不同年份都区分开的语言模型"></a>不同年份都区分开的语言模型</h3><img src="/images/pasted-81.png" alt="upload successful">  <h2 id="High-Dimensional-Outputs"><a href="#High-Dimensional-Outputs" class="headerlink" title="High-Dimensional Outputs"></a>High-Dimensional Outputs</h2>在自然语言处理中，通常会遇到输出维度很多的情况。为了减轻计算的复杂度，通常有下面的做法  </li>
<li>use of a short list，将输出限制在最常见的word(10000~20000)中  </li>
<li>hierachical softmax，这个存在的一个问题是如果数中的类不是很准确，得到的结果可能会很差  </li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://lpfan0307.com/2020/02/27/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chp10-Sequence-Modeling-Recurrent-and-Recursive-Nets/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="nullfan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="life of nullfan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/27/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chp10-Sequence-Modeling-Recurrent-and-Recursive-Nets/" itemprop="url"> Deep Learning读书笔记 chp10-Sequence Modeling: Recurrent and Recursive Nets</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-02-27T22:36:28+08:00">
                2020-02-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="RNN常见的几种结构"><a href="#RNN常见的几种结构" class="headerlink" title="RNN常见的几种结构"></a>RNN常见的几种结构</h1><h2 id="输入输出都是序列"><a href="#输入输出都是序列" class="headerlink" title="输入输出都是序列"></a>输入输出都是序列</h2><p><img src="/images/pasted-70.png" alt="upload successful"><br>$a^{(t)} = b + Wh^{(t-1)} + Ux^{(t)}$<br>$h^{(t)} = \tanh(a^{(t)})$<br>$o^{(t)} = c + Vh^t$<br>${\widehat y}^{(t)} = softmatx(o^{(t)})$  </p>
<p><img src="/images/pasted-71.png" alt="upload successful"><br>这个网络图将每一层的输出层传递给后面一层，这通常会丢失到一些信息； 相比传递隐藏层，这样的网络的每层传递的信息更少。但这样的网络在训练和并行上具有一定的便利性。  </p>
<h2 id="输入是序列，输出是一个单个"><a href="#输入是序列，输出是一个单个" class="headerlink" title="输入是序列，输出是一个单个"></a>输入是序列，输出是一个单个</h2><p><img src="/images/pasted-72.png" alt="upload successful">  </p>
<h2 id="Tearch-Forcing"><a href="#Tearch-Forcing" class="headerlink" title="Tearch Forcing"></a>Tearch Forcing</h2><p><img src="/images/pasted-73.png" alt="upload successful">  </p>
<h2 id="fully-connected-graphical-model"><a href="#fully-connected-graphical-model" class="headerlink" title="fully connected graphical model"></a>fully connected graphical model</h2><p><img src="/images/pasted-74.png" alt="upload successful">  </p>
<h2 id="输入是固定长度向量，输出是一个序列"><a href="#输入是固定长度向量，输出是一个序列" class="headerlink" title="输入是固定长度向量，输出是一个序列"></a>输入是固定长度向量，输出是一个序列</h2><p><img src="/images/pasted-75.png" alt="upload successful">  </p>
<h2 id="双向RNN"><a href="#双向RNN" class="headerlink" title="双向RNN"></a>双向RNN</h2><p><img src="/images/pasted-76.png" alt="upload successful">  </p>
<h2 id="Encoder-Decoder-Sequence-to-Sequence-Architectures"><a href="#Encoder-Decoder-Sequence-to-Sequence-Architectures" class="headerlink" title="Encoder-Decoder Sequence-to-Sequence Architectures"></a>Encoder-Decoder Sequence-to-Sequence Architectures</h2><p><img src="/images/pasted-77.png" alt="upload successful">  </p>
<h1 id="The-Long-Short-Term-Memory-and-Other-Gated-RNNs"><a href="#The-Long-Short-Term-Memory-and-Other-Gated-RNNs" class="headerlink" title="The Long Short-Term Memory and Other Gated RNNs"></a>The Long Short-Term Memory and Other Gated RNNs</h1><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p><img src="/images/pasted-78.png" alt="upload successful"> </p>
<h3 id="forget-gate"><a href="#forget-gate" class="headerlink" title="forget gate:"></a>forget gate:</h3><p>$$f_i^{(t)} = \sigma (b_i^f + \sum_j{(U_{i,j}^f x_j^{(t)})} + \sum_j{W_{(i,j)}^f h_j^{(t-1)})}$$<br>其中$x^{(t)}$是当前输入向量，$h^{(t)}$是当前隐藏层向量，$b^f$、$U_f$、$W^f$分别对应forget的bias、输入权重和循环权重 </p>
<h3 id="the-LSTM-cell-internal-state"><a href="#the-LSTM-cell-internal-state" class="headerlink" title="the LSTM cell internal state"></a>the LSTM cell internal state</h3><p>$$s_i^{(t)} = f_i^{(i)}s_i^{(t-1)} + g_i^{(t)} \sigma (b_i+ \sum_j U_{(i,j)}x_j^{(t)} + \sum_j W_{(i,j)}h_j^{(t-1)})$$<br>其中$b$、$U$、$W$分别代表LSTM cell的bias、输入权重和循环权重  </p>
<h3 id="the-external-input-gate"><a href="#the-external-input-gate" class="headerlink" title="the external input gate"></a>the external input gate</h3><p>$$g_i^{(t)} = \sigma (b_i^g + \sum_j U_{(i,j)}^g x_j^{(t)} + \sum_j W_{(i,j)}^g h_j^{(t-1)})$$  </p>
<h3 id="output-gate"><a href="#output-gate" class="headerlink" title="output gate"></a>output gate</h3><p>$$h_i^{(t)} = \tanh (s_i^{(t)}) q_i^{(t)}$$<br>$$q_i^{(t)} = \sigma (b_i^o + \sum_j U_{(i,j)}^o x_j^{(t)}+ \sum_j W_{(i,j)}^o h_j^{(t-1)})$$<br>output $h_i^{(t)}$由output gate $q_i^{(t)}$控制  </p>
<h2 id="GRN"><a href="#GRN" class="headerlink" title="GRN"></a>GRN</h2><p>$$h_i^{(t)} = u_i^{(t-1)} h_i^{(t-1)} + (1 - u_i^{(t-1)}) \sigma (b_i + \sum_j U_{(i,j)} x_j^{(t-1)}) + \sum_j W_{(i,j)} r_j^{(t-1)} h_j^{(t-1)})$$<br>$$u_i^{(t)} = \sigma (b_i^u + \sum_j U_i^u x_j^{(t)} + \sum_j W_{(i,j)}^u h_j^{(t)})$$<br>$$r_i^{(t)} = \sigma (b_i^r + \sum_j U_i^r x_j^{(t)} + \sum_j W_{(i,j)}^r h_j^{(t)})$$</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://lpfan0307.com/2020/02/24/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chp9-Convolutional-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="nullfan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="life of nullfan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/24/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chp9-Convolutional-Networks/" itemprop="url"> Deep Learning读书笔记 chp9-Convolutional Networks</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-02-24T10:34:30+08:00">
                2020-02-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="the-motivation"><a href="#the-motivation" class="headerlink" title="the  motivation"></a>the  motivation</h1><ul>
<li>sparse interactions<br>例如对于一张图片，有成百甚至上千个像素，但是对于发现一些边缘的基础特征通常只需要很少的像素，即卷积核不需要太大。另外，对于一个输入维度为$n$，输出维度为$m$的场景，通过卷积可以限制与输出连接维度为$k$。在实际应用场景中，通常$k$取比$m$小很多的值就可以达到不错的效果</li>
<li>parameter sharing<br>对于一张图片，使用卷积的方式我们通常不需要对每个局部区域学习不同的参数，而可以对这些局部学习一组共同的参数，这也可以减少需要存储的参数。</li>
<li>equivariant representations<br>对于图片的平移或者时序信息，使用卷积可以获得相同的输出。  </li>
</ul>
<h1 id="pooling"><a href="#pooling" class="headerlink" title="pooling"></a>pooling</h1><p>pooling操作是将网络的局部信息使用某些统计量来代替的方式。使用一些pooling操作后，对于输入的一些微小变动，几乎都可以保持不变的结果。在我们关注某些特生是否出现，在不在乎出现在什么的时候，pooling操作具有很好的作用。pool操作加入了一个很强的先验偏置：使用了pool层的必须对微小的变化保持不变。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://lpfan0307.com/2020/02/24/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chp8-Regularization-for-Deep-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="nullfan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="life of nullfan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/24/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chp8-Regularization-for-Deep-Learning/" itemprop="url"> Deep Learning读书笔记 chp8-Regularization for Deep Learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-02-24T00:13:20+08:00">
                2020-02-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="mini-batch"><a href="#mini-batch" class="headerlink" title="mini-batch"></a>mini-batch</h1><p>$n$个样本的均值的标准差为$\sigma / \sqrt{n }$，分母的$\sqrt n$表明样本的均值的标准差的下降程度小于线性。考虑到100个样本和10000个样本，后者计算程度是前者的100倍，但准确程度却仅是10倍。从总体计算复杂度来说，使用mini-batch是收敛更快的方式。  </p>
<h1 id="Nesterov-Momentum"><a href="#Nesterov-Momentum" class="headerlink" title="Nesterov Momentum"></a>Nesterov Momentum</h1><p>在计算梯度时，将$\theta$加上累计的梯度方向，在凸函数全局梯度下，可以将下降速度从$O(1/k)$变为$O(1/k^2)$<br><img src="/images/pasted-69.png" alt="upload successful"></p>
<h1 id="Batch-Nomalization"><a href="#Batch-Nomalization" class="headerlink" title="Batch Nomalization"></a>Batch Nomalization</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://lpfan0307.com/2020/02/23/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chp7-Regularization-for-Deep-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="nullfan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="life of nullfan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/23/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chp7-Regularization-for-Deep-Learning/" itemprop="url">Deep Learning读书笔记 chp7-Regularization for Deep Learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-02-23T23:20:12+08:00">
                2020-02-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="正则化的几种方式"><a href="#正则化的几种方式" class="headerlink" title="正则化的几种方式"></a>正则化的几种方式</h1><h2 id="惩罚项和约束项"><a href="#惩罚项和约束项" class="headerlink" title="惩罚项和约束项"></a>惩罚项和约束项</h2><p>借助广义拉格朗日算法，惩罚项和约束项在一定程度上可以看做是等价的。  </p>
<h2 id="数据增广（data-augmentation）"><a href="#数据增广（data-augmentation）" class="headerlink" title="数据增广（data augmentation）"></a>数据增广（data augmentation）</h2><p>数据增光对于一些特定的问题可以取得很好的效果，例如在物体识别中，将图片的每个像素沿着某个方向旋转，将新获得的图片加入训练集，通常可以获得很好的效果，即便是在使用的卷积这样对旋转不变的操作后。</p>
<h2 id="噪声鲁棒性（noise-robustnesss）"><a href="#噪声鲁棒性（noise-robustnesss）" class="headerlink" title="噪声鲁棒性（noise robustnesss）"></a>噪声鲁棒性（noise robustnesss）</h2><p>一般来说，噪声的引入比单纯的对参数的shrinking要重要，特别是这些噪声是加在隐层单元上时。</p>
<h2 id="multi-task-learning"><a href="#multi-task-learning" class="headerlink" title="multi-task learning"></a>multi-task learning</h2><p>多任务学习模型的架构：<br>屏幕快照 2020-02-23 下午11.37.16<br><img src="/images/pasted-66.png" alt="upload successful"><br>多任务学习中，一个模型的参数会由两个部分组成（1）模型特有的参数（2）几个模型通用的参数。可以使用多任务学习的一个前提是不同的任务之间存在某些统计学上的关系。</p>
<h2 id="early-stopping"><a href="#early-stopping" class="headerlink" title="early stopping"></a>early stopping</h2><h2 id="bagging-and-other-ensemble-methods"><a href="#bagging-and-other-ensemble-methods" class="headerlink" title="bagging and other ensemble methods"></a>bagging and other ensemble methods</h2><p>将几组模型平均通常比单个模型取得更好的效果，这背后的原因是因为不同的模型通常不会犯相同的错误。  </p>
<p><img src="/images/pasted-68.png" alt="upload successful">  </p>
<h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2><p>dropout和bagging的关联：（1）bagging的各个模型的参数是独立的，但dropout的各个模型的参数是从母shenjingwangluo继承的不同子集，（2）bagging的每个模型都可以被充分训练，而dropout的模型数太多以至于让每个模型充分训练是不太现实的，但各个模型之间的参数共享可以使得各个模型都取得一个不错的结果。（3）除此之外，bagging和dropout是一样的。<br>dropout有的优点：</p>
<ul>
<li>计算更加轻便  </li>
<li>对神经网络的结构没有要求，通用性很广<br>dropout取得很好的结果，很大一部分是来自于给隐层节点的噪声。例如，如果需要识别一个人脸，一个隐藏节点通过鼻子学到该脸部的信息，当这个节点被随机丢弃时，就迫使神经网络去学习其他如嘴巴、耳朵等信息。而传统的噪声引入很难做到将一个鼻子或者嘴巴丢掉这样。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://lpfan0307.com/2020/02/20/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chp5-Machine-Learning-Basics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="nullfan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="life of nullfan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/20/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chp5-Machine-Learning-Basics/" itemprop="url">Deep Learning读书笔记 chp6-Deep Feedforward Networks</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-02-20T18:48:40+08:00">
                2020-02-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何捕捉非线性关系-phi"><a href="#如何捕捉非线性关系-phi" class="headerlink" title="如何捕捉非线性关系$\phi$"></a>如何捕捉非线性关系$\phi$</h1><p>1、选取一个非常通用的$\phi$，例如RBF核使用的无限维度的$\phi$。使用这样的通常会有足够的能力来适应训练集，但通常不会取得很好的泛化误差。非常通用的特征映射通常是基于局部平滑性，没有引入足够的先验的偏置的信息<br>2、人工映射$\phi$。在深度学习兴起之前，自然语言、图像处理、语音识别领域做了分别做了很多努力，但这些对应的编映射的迁移能力很差，没有通用性<br>3、深度学习采用的方式是学习这样的$\phi$。这样的一个好处是，我们只需要定义一个宽泛的函数簇，交给神经网络去学习，而不是从找到一个很精确的函数簇  </p>
<p>choose the optimizer<br>the cost function<br>the form of output<br>activation functions</p>
<h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><h2 id="relu"><a href="#relu" class="headerlink" title="relu"></a>relu</h2><p>在现代神经网络中，默认的激活函数一般选取Relu（rectified linear unit）。使用Relu可以获得非线性变换。由于Relu非常接近线性函数，可以保留许多线性函数使用随机梯度下降的优势，同时也保留了线性函数泛化好的特性。Relu在其定义域中的一半都是线性函数，这使得在其激活（active）时，其导数都很大。</p>
<h2 id="sigmod"><a href="#sigmod" class="headerlink" title="sigmod"></a>sigmod</h2><p>Sigmod函数$g(z) = /sigma (z)$，当$z$为比较的负数的时候，函数本身值会非常小；并且只有函数值只有$z$在0附近的时候才非常敏感，所以sigmod函数当做前馈神经网络隐层的激活函数是不推荐的。sigmod在某些特定的时刻可以作为输出层，比如当损失函数可以抵消simod函数的saturation时。<br>Sigmod在非前馈神经网络通常使用得更加频繁。例如RNN、autoencoders等神经网络，这些神经网络排除了线性模型，所以还是会更多的使用sigmod函数尽管依然存在saturation。  </p>
<h2 id="某些层可以使单纯的线性函数"><a href="#某些层可以使单纯的线性函数" class="headerlink" title="某些层可以使单纯的线性函数"></a>某些层可以使单纯的线性函数</h2><p>单纯使用线性函数可以起到压缩神经网络的效果。考虑一个有$n$个输入和$p$个输出的神经网络，如果直接映射，需要np个参数；如果中间加上一层的输出有$q$个参数，那么总共是需要$(n + p)q$个参数。对于比较小的$q$，可以起到很好的压缩空间的效果。这可以理解为将输入映射到一个低维空间，但通常很小的低维空间也可以表示很多原始的信息。</p>
<h1 id="损失函数的选取"><a href="#损失函数的选取" class="headerlink" title="损失函数的选取"></a>损失函数的选取</h1><p>损失函数的梯度必须足够大以便于给学习算法提供一些指导。<br>negative log-likelihood可以帮助很多模型解决梯度过小的问题。<br>MSE和MAE不适合作为损失函数，因为他们和其他输出层和这些损失函数一起的时候，通常会导致很小的梯度。 </p>
<h1 id="神经网络架构的设计"><a href="#神经网络架构的设计" class="headerlink" title="神经网络架构的设计"></a>神经网络架构的设计</h1><p>更深的神经网络通常每层需要的单元和参数更少，泛化能力更强，但也更难被优化。<br>屏幕快照 2020-02-23 下午10.28.00<br><img src="/images/pasted-65.png" alt="upload successful">  </p>
<p>使用更深的神经网络通常基于这样一个信念：我们需要学习的函数是由很多简单的函数组成的。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://lpfan0307.com/2020/02/12/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chp4-Numerical-Computation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="nullfan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="life of nullfan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/12/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chp4-Numerical-Computation/" itemprop="url">Deep Learning读书笔记 chp5-Machine Learning Basics</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-02-12T00:07:00+08:00">
                2020-02-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">读书笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="相对传统机器学习，驱动Deep-Learning发展的主要因素是什么"><a href="#相对传统机器学习，驱动Deep-Learning发展的主要因素是什么" class="headerlink" title="相对传统机器学习，驱动Deep Learning发展的主要因素是什么?"></a>相对传统机器学习，驱动Deep Learning发展的主要因素是什么?</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">维度灾难</span><br></pre></td></tr></table></figure>
<p>许多传统机器学习通常假设一个新的样本可以在训练样本中找到很相近的样本，但当特征维度很多的时候，样本在空间的分布是非常稀疏的，需要新遇到的样本可能无法找到相邻的样本。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">平滑假设或者局部一致性假设的实效</span><br></pre></td></tr></table></figure>
<p>很多机器学习算法都存在隐藏的先验偏置：平滑假设或者局部一致性假设，即如果两个样本比较接近，那么他们对应的输出也比较接近。常见的$k$近邻算法、决策树算法都包含这样的先验偏置。<br>在考虑仅仅平滑先验或者局部一致性先验偏置的情况下，那么最终空间划分的数目是没有办法多过样本的数目的。考虑一个棋盘，棋盘上黑白子相间，仅仅考虑平滑先验或者局部一致性先验，是没有办法对一个新见到的棋子进行预测的。但是如果我们加入周期性先验，就可以很好的对棋盘棋子的颜色进行预测。<br>在深度学习中，一个核心的思想是，我们认为事物都是由一个个关键因素组成的，即一层层抽样而成的。<br><img src="/images/pasted-64.png" alt="upload successful">  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">流形假设</span><br></pre></td></tr></table></figure>
<p>流形是一个连接的区域。<br>流形假设：我们通常不需要关注整个$\mathbb R^n$空间的所有区域，我们需要关注的空间很少的几部分互相连接的点，即流形<br>有两点观察可以说明在某些情况下流形假设是成立（1）图像、文本的概率分布函数是非常集中的。随机生成的字很难构成句子，随机像素很难构成图。(2)那些有效的点是互相连接的。考虑一张图片，我们对他进行变暗、变亮、旋转等各种操作，得到的还是一张有效的图。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://lpfan0307.com/2020/02/03/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="nullfan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="life of nullfan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/03/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" itemprop="url">Deep Learning读书笔记 chp3-Probability and Information Theory</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-02-03T21:47:33+08:00">
                2020-02-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="概率存在的三个主要因素"><a href="#概率存在的三个主要因素" class="headerlink" title="概率存在的三个主要因素"></a>概率存在的三个主要因素</h1><ul>
<li>系统内在的随机性；如量子力学或者扑克牌  </li>
<li>观察不完备</li>
<li>建模不完备  </li>
</ul>
<p>很多时候，使用一个简单的不确定性的例子，通常比使用一个确定性的但复杂很多的例子更有意义，例如：“大多数鸟会飞” VS “鸟会飞，除了那些还没有学会飞行的幼鸟、受伤的或者虚弱的丧失了飞行能力的鸟、还有一些特殊种类的鸟比如鹤鸵等等”<br>概率可以看错是逻辑在不确定性上的扩展  </p>
<h1 id="概率分布"><a href="#概率分布" class="headerlink" title="概率分布"></a>概率分布</h1><p>随机变量：$\mathrm{x}$(一般用罗马字体)<br>随机变量对应的概率分布：$\mathrm{x} ~ P(\mathrm{x})$  </p>
<h1 id="概率密度函数（probability-density-function-PDF-）"><a href="#概率密度函数（probability-density-function-PDF-）" class="headerlink" title="概率密度函数（probability density function(PDF)）"></a>概率密度函数（probability density function(PDF)）</h1><p>概率密度函数有如下性质:  </p>
<ul>
<li>$p$的值域是$\mathrm{x}$所有可能的状态  </li>
<li>$\forall x \in \mathrm{x}, p(x) \ge 0$，注意这里没有$p(x) \le 1$的约束  </li>
<li>$\int{p(x)d(x)} = 1$  </li>
</ul>
<h1 id="边缘概率（marginal-probability）"><a href="#边缘概率（marginal-probability）" class="headerlink" title="边缘概率（marginal probability）"></a>边缘概率（marginal probability）</h1><p>对于离散值：<br>$\forall x \in \mathrm{x}, P(\mathrm{x} = x) = \underset{y} \sum P(\mathrm{x}=x,\mathrm{y}=y)$<br>对于连续值：<br>$p(x) = \int{p(x,y)dy}$  </p>
<h1 id="条件概率（conditional-probability）"><a href="#条件概率（conditional-probability）" class="headerlink" title="条件概率（conditional probability）"></a>条件概率（conditional probability）</h1><p>$P(\mathrm{y} = y |\mathrm{x} = x) = \frac{P(\mathrm{x} = x, \mathrm{y} = y)}{P(\mathrm{x} = x)}$ </p>
<h1 id="条件概率的链式规则-the-chain-rule-of-conditional-probabilities"><a href="#条件概率的链式规则-the-chain-rule-of-conditional-probabilities" class="headerlink" title="条件概率的链式规则(the chain rule of conditional probabilities)"></a>条件概率的链式规则(the chain rule of conditional probabilities)</h1><p>$P(\mathrm{x}^{(1)},\mathrm{x}^{(2)},…,\mathrm{x}^{(n)}) = P(\mathrm{x}^{(1)}) \prod_{i=2}^nP(\mathrm x^{(i)} | \mathrm{x}^{(1)},…,\mathrm x^{(i-1)})$<br>$P(\mathrm {a,b,c}) = P(\mathrm {a|b,c})P(\mathrm {b,c})$<br>$P(\mathrm {b,c}) = P(\mathrm {b|c})P(\mathrm c)$<br>$P(\mathrm {a,b,c}) = P(\mathrm {a|b,c})P(\mathrm {b|c})P(\mathrm c)$</p>
<h1 id="条件独立-conditional-independence"><a href="#条件独立-conditional-independence" class="headerlink" title="条件独立(conditional independence)"></a>条件独立(conditional independence)</h1><p>$x$和$y$条件独立，则有:$\forall x \in \mathrm x, y \in \mathrm y, p(\mathrm x = x, \mathrm y = y) = p(\mathrm x = x)p(\mathrm y = y)$  </p>
<h1 id="期望-expection-、方差-variance-、协方差-covariance"><a href="#期望-expection-、方差-variance-、协方差-covariance" class="headerlink" title="期望(expection)、方差(variance)、协方差(covariance)"></a>期望(expection)、方差(variance)、协方差(covariance)</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">期望</span><br></pre></td></tr></table></figure>
<p>对于离散值，$\mathbb E_{\mathrm x \sim P[f(x)]} = \underset{x} \sum P(x)f(x)$<br>对于连续值，$\mathbb E_{\mathrm x \sim p[f(x)]} = \inf p(x)f(x)dx$  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">方差</span><br></pre></td></tr></table></figure>
<p>$Var(f(x)) = \mathbb E[(f(x) - \mathbb E[f(x)])^2]$  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">协方差</span><br></pre></td></tr></table></figure>
<p>$Cov(f(x), g(y)) = \mathbb E[(f(x)- \mathbb E[f(x)])(g(y)- \mathbb E[g(y)])] $  </p>
<h1 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h1><p>信息论背后直观的想法如下：不经常发生的事情往往比经常发生的事情更加具有信息量，比如“太阳今天升起了”没有什么信息量，而“今天发生日食了”就更加具有信息量<br>我们希望找到一个符合上述直觉的衡量信息的方式  </p>
<ul>
<li>更可能发生的事情具有更少的信息量，极端情况下，确定发生的事情没有信息量  </li>
<li>不常发生的事情具有更多的信息量  </li>
<li>独立的事情具有额外的信息量<br>为了满足上面三个性质，我们定义<strong>自信息(self-information)</strong><br>$$I(x) = -\log P(x)$$</li>
</ul>
<h1 id="KL散度（Kullback-Leibler-KL-divergence）"><a href="#KL散度（Kullback-Leibler-KL-divergence）" class="headerlink" title="KL散度（Kullback-Leibler (KL) divergence）"></a>KL散度（Kullback-Leibler (KL) divergence）</h1><p>如果存在对于$\mathrm x$的$P(x)$和$Q(x)$两个不同的概率分布，那么我么可以KL散度来衡量这两个分布的差别<br>$$D_{KL}(P||Q) = E_{\mathrm x \sim P} [\log \frac{P(x)}{Q(x)}]$$  </p>
<h1 id="图概率模型"><a href="#图概率模型" class="headerlink" title="图概率模型"></a>图概率模型</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">有向图</span><br></pre></td></tr></table></figure>
<p><img src="/images/pasted-61.png" alt="upload successful"><br>$p(a,b,c,d,e) = p(a)p(b|a)p(c|a,b)p(d|b)p(e|c)$  </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://lpfan0307.com/2020/01/20/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chp2-Linear-Algebra/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="nullfan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="life of nullfan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/20/Deep-Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chp2-Linear-Algebra/" itemprop="url">Deep Learning读书笔记 chp2-Linear Algebra</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-01-20T11:20:00+08:00">
                2020-01-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">读书笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Deep Learning第二章，线性代数，这里我摘抄一些重要的公式，顺便练习下latex  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">矩阵乘法的分配律和结合律</span><br></pre></td></tr></table></figure>
<ul>
<li>$A(B+C) = AB + AC$</li>
<li>$A(BC) = (AB)C$  </li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Norms</span><br></pre></td></tr></table></figure>
<p>$L^p $ Norm如下：<br>$||x||_p = (\sum_i{|x_i|}^p)^{1 \over p}$  </p>
<p>Norm有如下性质：</p>
<ul>
<li>$f(x) = 0 \Rightarrow x = 0$  </li>
<li>$f(x + y) \leq f(x) + f(y) $(the triangle inequality)</li>
<li>$\forall \alpha \in \mathbb{R}, f({\alpha}x) = |\alpha|f(x)$</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">正交矩阵(orthogonal matrix)</span><br></pre></td></tr></table></figure>
<p>单位矩阵(unit vector)：$||x||_2 = 1$<br>正交矩阵：$A^TA = AA^T = I$，这意味着 $A^{-1} = A^T$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">正定矩阵(positive definite)</span><br></pre></td></tr></table></figure>
<p>所有的特征值都为整数的矩阵是正定矩阵(positive define)，所有特征值都为非负数的矩阵是半正定矩阵(positive semidefine)<br>所有的半正定矩阵存在如下性质：${\forall} x, x^TAx \geq 0$<br>正定矩阵还有额外性质：$x^TAx = 0 \Rightarrow x = 0$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">特征分解（eigendecomposition）</span><br></pre></td></tr></table></figure>
<p>$A$的特征向量表示：<br>$Av = {\lambda}v$，其中$v$为非0向量，表示$A$的特征向量，$\lambda$为对应特征值<br>由于$v$的每个元素放大$k$倍，还是$A$的特征向量，所以通常情况是考虑$v$为单位向量的情况<br>假设$A$有$n$个线性无关特征向量，表示为 $V = [v^{(1)}, … , v^{(n)}]$，对应的特征值表示为 $\lambda = [{\lambda}^{(1)}, … , {\lambda}^{(n)}]$，则矩阵$A$的特征分解表示为$$A = V{diag(\lambda)}V^{-1}$$，并不是所有的矩阵都有对应的特征分解。  </p>
<p>实数对称矩阵有一个很好的性质，每个实数对称矩阵都可以被分解为如下：$$A = Q{\Lambda}Q^T$$，其中$Q$是由矩阵$A$的特征向量组成的正交矩阵，$\Lambda$是由$A$的特征值组成的对角矩阵  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">奇异值分解(sigular value decomposition)</span><br></pre></td></tr></table></figure>
<p>有些矩阵不存在特征分解，但任何矩阵都存在奇异值分解，$A = UDV^T$<br>假设$A$是$m \times n$的矩阵，则$U$是$m \times m$的正交矩阵，$D$是对角矩阵，$V$也是正交矩阵<br>更进一步，可以理解为$U$是$AA^T$的特征向量，$V$是$A^TA$的特征向量，非零的奇异值是$A^TA$的平方根  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">摩尔-彭若斯广义逆(The Moore-Penrose Pseudoinverse)</span><br></pre></td></tr></table></figure>
<p>对于$Ax = y$,我们希望可以找到一个矩阵$B$，左乘以后希望可以得到$x = By$<br>对于不同特性的$A$，可能不存在这样的矩阵$B$或者是存在多个这样的矩阵$B$<br>摩尔-彭若斯广义逆定义如下：$A^+ = \lim_{\alpha \searrow 0}(A^TA + {\alpha}I)^{-1}A^T$<br>实际应用中通常采用另一种方式计算广义逆 $A^+ = VD^+U^T$，其中$U$、$V$、$D$是$A$的奇异值分解对应的矩阵，$D^+$是通过$D$的非零元素取倒数得到的矩阵转置得到<br>当$A$的列数多余行数时，广义逆是无数可能解法中的一种，其中 $x = A^+y$是所有解法中$x$的二范数$(||x||_2)$最小的解<br>当$A$的行数多余列数时，左乘的矩阵的解有可能不存在。在这种情况下，使用广义逆得到的$x$，会使得$Ax$和$y$在欧氏距离上最小$(||Ax-y||_2)$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">矩阵的迹(The Trace Operator)</span><br></pre></td></tr></table></figure>
<p>$Tr(A) = \sum_i{A_{i,i}}$</p>
<p>矩阵的迹有如下性质:</p>
<ul>
<li>$||A||_F = \sqrt{AA_T}$</li>
<li>$Tr(A) = Tr(A^T)$  </li>
<li>$Tr(ABC) = Tr(CBA) = Tr(BCA)$  </li>
<li>$Tr(AB) = Tr(BA)$  </li>
<li>$a = Tr(a)$  </li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">行列式(the Determinant)</span><br></pre></td></tr></table></figure>
<p>$det(A)$为矩阵的所有特征值的乘积<br>行列式的绝对值表示与该矩阵相乘时，被乘矩阵在空间延展或者浓缩的程度。如果矩阵的行列式为0，乘完后失去所有体积； 如果矩阵行列式绝对值为1，乘完后保持体积不变。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">主成分分析(prinipal components analysis)</span><br></pre></td></tr></table></figure>
<p>主成分分析需要做的事情是这样的，假设我们有一个$n$维的数据，即$x \in {\mathbb{R}}^n$，我们希望可以找到一个映射关系f，将$x$映射到$l$维中，即$f(x) = c$，$c \in {\mathbb{R}}^l，$其中$l \lt n$。同时，在映射的过程中保持信息损失的信息尽可能的少，即我们可以找到另一个映射关系$g$，将$c$映射为$n$维的空间，映射回来的结果要和原始结果尽量接近，即$x \approx g(f(x))$。主成分分析就是希望可以找到$f$和$g$<br>为了简化问题，我们假设$g(c)$的对应的解码方式如下$g(c) = Dc$，其中$D \in {\mathbb{R}}^{n \times l}$<br>为了是问题简便，主成分分析时加了两个约束条件  </p>
<ul>
<li>矩阵D的每列互相正交  </li>
<li>矩阵D的每列都是单位向量 </li>
</ul>
<p>主成分分析的主要思想是$x \approx g(f(x))$，如果采用二范式，</p>
<p>则为$c^* = \underset{c}{\arg \min} ||x -g(c)||_2$，</p>
<p>等价于$c^* = \underset{c}{\arg \min} ||x -g(c)||_2^2 $   </p>
<p>$||x -g(c)||_2^2 = {(x-g(c))}^T(x-g(c))$ $=$ $x^Tx - x^Tg(c) - {g(c)}^Tx + {g(c)}^Tg(c)$ =  $x^Tx - 2x^Tg(c) + {g(c)}^Tg(c)$<br>去除和$c$无关的部分，$c^* =\underset{c}{\arg \min} - 2x^Tg(c) + {g(c)}^Tg(c)$  </p>
<p>将$g(c) = Dc$代入，$c^* =\underset{c}{\arg \min} - 2x^TDc + {Dc}^TDc$ </p>
<p>$=\underset{c}{\arg \min} - 2x^TDc + c^TI_lc$<br>$=\underset{c}{\arg \min} - 2x^TDc + c^Tc$<br>上式对$c$求导，即 $-2D^Tx + 2c = 0 \Rightarrow c = D^Tx$<br>上述推导表明，对于给定的$x,D$，$c$取$D^Tx$是目标的最优解<br>又知道$c = f(x)$，则有$f(x) = c = D^Tc$, $r(x) = g(f(x)) = g(c) = Dc = DD^Tc$<br>考虑所有的样本，则最终表达式为$D^* = \underset{D} \arg \min \sum_{i,j} {(x_j^{(i)} - r(x^{(i)})_j)}^2,, s.t. D^TD = I_l$<br>考虑$l=1$的情况，上面的式子化简为   </p>
<p>$d^* = \underset{d} \arg \min \underset{i} \sum ||x^(i) - dd^Tx^{(i)}||_2^2, s.t. ||d||_2 = 1$    </p>
<p>考虑所有样本，$d^* = \underset{d} \arg \min \underset{i} \sum ||X - dd^TX||_2^2, s.t. ||d||_2 = 1$  </p>
<p>$\underset{d} \arg \min ||X - Xdd^T||_F^2 s.t. d^d = 1$ </p>
<p>不考虑约束条件 </p>
<p>$\underset{d} \arg \min ||X - Xdd^T||_F^2$  </p>
<p>$=\underset{d} \arg \min Tr((X - Xdd^T)^T(X-Xdd^T))$<br>$=\underset{d} \arg \min Tr(X^TX - X^TXdd^T - dd^TX^TX + dd^TX^TXdd^T)$<br>$=\underset{d} \arg \min Tr(- X^TXdd^T - dd^TX^TX + dd^TX^TXdd^T)$(去除不包含d的项)<br>$=\underset{d} \arg \min -Tr( X^TXdd^T) - Tr(dd^TX^TX) + Tr(dd^TX^TXdd^T)$<br>$=\underset{d} \arg \min -2Tr( X^TXdd^T)  + Tr(dd^TX^TXdd^T)$<br>$=\underset{d} \arg \min -2Tr( X^TXdd^T)  + Tr(X^TXdd^Tdd^T)$<br>$=\underset{d} \arg \min -2Tr( X^TXdd^T)  + Tr(X^TXdd^T)$ （$d^Td = 1$）<br>$=\underset{d} \arg \min -Tr( X^TXdd^T)  $ （$d^Td = 1$）<br>$=\underset{d} \arg \max Tr( X^TXdd^T)  $ （$d^Td = 1$）<br>$=\underset{d} \arg \max Tr( d^TX^TXd)  $ （$d^Td = 1$）  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">参考资料</span><br></pre></td></tr></table></figure>
<p>latex数学公式在线编辑器：所见即所得  <a href="https://zohooo.github.io/jaxedit/" target="_blank" rel="noopener">https://zohooo.github.io/jaxedit/</a><br>latex输入语法：<a href="https://zhuanlan.zhihu.com/p/50667788" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/50667788</a><br>latex箭头汇总:<a href="https://blog.csdn.net/liyuanbhu/article/details/51473886" target="_blank" rel="noopener">https://blog.csdn.net/liyuanbhu/article/details/51473886</a><br>pac讲解视频：<a href="https://www.youtube.com/watch?v=FgakZw6K1QQ" target="_blank" rel="noopener">https://www.youtube.com/watch?v=FgakZw6K1QQ</a><br>latex特殊符号：<a href="https://blog.csdn.net/caiandyong/article/details/53351737" target="_blank" rel="noopener">https://blog.csdn.net/caiandyong/article/details/53351737</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">&lt;i class&#x3D;&quot;fa fa-angle-right&quot;&gt;&lt;&#x2F;i&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">nullfan</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">categories</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">nullfan</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
